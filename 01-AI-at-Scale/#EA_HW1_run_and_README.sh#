#
#
# HW1 E. D. Araya
#
#
# Problem A
echo "****** Problem A: "
echo "-Instructions: Make sure that the single GPU code runs on Polaris."
echo "  Code provided in the lecture was copied to a .py script,"
echo "  edited and a .sh script was modified to run it"
echo "-Solution: The code successfully runs" 
#bash EA_HW1_A_run_pytorch_single_GPU_example.sh

# Problem B
echo "****** Problem B:" 
echo "-Instructions: The counting of ranks, does not necessarily has to be"
echo "  a mix-and-match between mpi4py and PALS. Try to implement the rank"
echo "  counting method using just PALS or mpi4py.device_count() methods can be useful here."
echo "-Solution: Checking online, there seems to be no mpi4py.device_count(), thus" 
echo "  the hint given in the instructions was unclear. torch.cuda.device_count() was"
echo "  used instead."
#bash EA_HW1_B_run_ddp_pytorch_2p8_N1_R4.sh

# Problem C
echo "***** Problem C:"
echo "-Instructions: Play with different dimensions of the src and tgt tensors."
echo "-Solution: the script pytorch_2p8_ddp.py was modified as follows: "
echo "  src:(2048, 1, 512), tgt:(2048, 20, 512) = total train time: 4.62s"
echo "  src:(4096, 1, 512), tgt:(4096, 20, 512) = total train time: 9.01s"
echo "  Computation time increased slightly slower than linear. "
echo "   **** src:(2048, 1, 512), tgt:(2048, 20, 512) ****"
#bash run_ddp_pytorch_2p8_N1_R4.sh
echo "   **** src:(4096, 1, 512), tgt:(4096, 20, 512) **** "
#bash EA_HW1_C_run_ddp_pytorch_2p8_N1_R4_TEST1.sh
echo "   **** Several tests were tried with higher dimensions, e.g.,"
echo "   src:(2048,2048, 1, 512), tgt:(2048,2048, 20, 512) that resulted in errors. "
echo "   It was unclear how to fix them."
#bash EA_HW1_C_run_ddp_pytorch_2p8_N1_R4_TEST2.sh

# Problem D
echo "***** Problem D:"
echo "-Instructions: Explore the cost of collective communication, by setting up"
echo "   a scenario, where you have only two ranks, but each rank resides on a "
echo "   different node. Profile and try to reason about the results." 
echo "-Solution: A copy of the script run_ddp_prof_pytorch_2p8_N1_R4.sh was"
echo "   created, then NODES was changed to 2. The script ran, and resulted in"
echo "   x3203c0s13b1n0.hsn.cm.polaris.alcf.anl.gov 0: total train time: 10.81s"
echo "   The original run_ddp_prof_pytorch_2p8_N1_R4.sh resulted in total train time: 12.12s,"
echo "   so it seems the NODES=2 improves efficiency, although as expected, collective "
echo "   communication would make the improvements significantly less than linear."
#bash EA_HW1_D_run_ddp_prof_pytorch_2p8_N1_R4.sh

# Problem E
echo "***** Problem D:"
echo "-Instructions: ***** Problem D:"
echo "-Instructions: 


